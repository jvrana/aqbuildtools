{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DASi to Aquarium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login to Benchling & Aquarium\n",
    "from aqbt.tools import config_to_sessions\n",
    "from aqbt.tools import parse_config\n",
    "import toml\n",
    "\n",
    "def config(config_path):\n",
    "    with open(config_path, \"r\") as f:\n",
    "        return parse_config(toml.load(f))\n",
    "\n",
    "\n",
    "def sessions(config):\n",
    "    return config_to_sessions(config)\n",
    "\n",
    "\n",
    "sessions = sessions(config('config.toml'))\n",
    "\n",
    "benchling = sessions['default']['benchling']\n",
    "registry = sessions['default']['registry']\n",
    "session = sessions['default']['aquarium']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aquarium Sample Recycler\n",
    "\n",
    "This class recycles samples that have been `trashed` (aka have \"TRASHED\" prefixed in their name).\n",
    "\n",
    "Call `recycler = SampleRecycler(session)` to create a new instance. \n",
    "\n",
    "To grab a trashed sample, call `recycler.reuse(sample_type_id)` to choose a trashed sample. A valid trashed sample\n",
    "should have no items, should not be used in any planned operation as an input, should not be used as a property in any field value for a non-trashed sample, and should have no field value properties.\n",
    "\n",
    "To trash a new sample, `recycler.trash_sample(sample)` which will append \"TRASHED\" prefix to its name and delete all of its field values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleRecycler(object):\n",
    "    \n",
    "    TRASHED = 'TRASHED'\n",
    "    USER_ID = 66\n",
    "    \n",
    "    def __init__(self, session):\n",
    "        self.session = session()\n",
    "        self.session.browser.model_cache = {}\n",
    "        self.session.using_cache = False\n",
    "        self._reused = []\n",
    "        self._trash = None\n",
    "    \n",
    "    @classmethod\n",
    "    def allowed(cls, sample, do_raise: bool = True):\n",
    "        if sample.user_id != cls.USER_ID:\n",
    "            if do_raise:\n",
    "                raise ValueError(\"Opps! Cannot access other users samples!\")\n",
    "            else:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    @classmethod\n",
    "    def is_trashed(cls, sample):\n",
    "        if not cls.allowed(sample, do_raise=False):\n",
    "            return False\n",
    "        elif sample.name.startswith(cls.TRASHED):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def trash_sample(self, sample):\n",
    "        if not self.can_be_trashed(sample):\n",
    "            print(\"Cannot trash sample\")\n",
    "            return\n",
    "        if not self.is_trashed(sample):\n",
    "            sample.name = \"TRASHED__\" + sample.name\n",
    "            sample.description = json.dumps({\n",
    "                'is_trashed': True,\n",
    "                'trashed_on': str(arrow.now()),\n",
    "                'previous_name': sample.name\n",
    "            })\n",
    "            \n",
    "            self.clear_field_values(sample)\n",
    "            self.trash.append(sample)\n",
    "            try:\n",
    "                sample.save()\n",
    "            except:\n",
    "                sample.name += str(uuid4())[-6:]\n",
    "                sample.save()\n",
    "        \n",
    "    def iter_trash(self):\n",
    "        for s in self.session.Sample.where({'user_id': self.USER_ID}):\n",
    "            if self.is_trashed(s):\n",
    "                if s.user_id != self.USER_ID:\n",
    "                    raise ValueError(\"Opps! Cannot access other user's samples!\")\n",
    "                yield s\n",
    "    \n",
    "    @classmethod\n",
    "    def clear_field_values(cls, sample):\n",
    "        if not cls.is_trashed(sample):\n",
    "            raise ValueError(\"Opps! Accidentally tried to clear a non trashed sample!\")\n",
    "        for fv in sample.field_values:\n",
    "            print(fv.value)\n",
    "            print(fv.sample)\n",
    "            do_save = False\n",
    "            if fv.value:\n",
    "                fv.value = None\n",
    "                do_save = True\n",
    "            if fv.child_sample_id:\n",
    "                fv.child_sample_id = None\n",
    "                do_save = True\n",
    "            if fv.sample:\n",
    "                fv.sample = None\n",
    "                do_save = True\n",
    "            if do_save:\n",
    "                fv.save()\n",
    "        \n",
    "    def force_clear_field_values(self):\n",
    "        for s in self.trash:\n",
    "            cleared = True\n",
    "            for fv in s.field_values:\n",
    "                if fv.value or fv.sample:\n",
    "                    print(fv.name)\n",
    "                    print('\\t {} {} {}'.format(fv.name, fv.value, fv.sample))\n",
    "                    cleared = False\n",
    "            if not cleared:\n",
    "                print(s.name)\n",
    "                self.clear_field_values(s)\n",
    "        \n",
    "    @property\n",
    "    def trash(self):\n",
    "        if self._trash is None:\n",
    "            samples = list(self.iter_trash())\n",
    "        \n",
    "            with self.session.with_cache() as sess:\n",
    "                sess.browser.update_cache(samples)\n",
    "                sess.browser.get(samples, 'items')\n",
    "                fvs = sess.FieldValue.where({'child_sample_id': [_t.id for _t in samples]})\n",
    "                sess.browser.get(fvs, {'operation', 'parent_sample'})\n",
    "                \n",
    "            self._trash = [s for s in samples if self.valid_for_reuse(s)]\n",
    "        return self._trash\n",
    "    \n",
    "    def _has_items(self, sample):\n",
    "        if sample.items:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def _is_used_in_field_value(self, sample, fvs = None):\n",
    "        with sample.session.with_cache(using_models=True) as sess:\n",
    "            fvs = sess.FieldValue.where({'child_sample_id': sample.id})\n",
    "            for fv in fvs:\n",
    "                if fv.role == 'input':\n",
    "                    return True\n",
    "                elif fv.role == 'output':\n",
    "                    return True\n",
    "            for fv in fvs:\n",
    "                if fv.parent_class == 'Sample':\n",
    "                    if not self.allowed(fv.parent_sample):\n",
    "                        return True\n",
    "                    elif not self.is_trashed(fv.parent_sample):\n",
    "                        return True\n",
    "        \n",
    "    def can_be_trashed(self, sample):\n",
    "        if not self.allowed(sample, do_raise=False):\n",
    "            return False\n",
    "        if not self._is_used_in_field_value(sample):\n",
    "            return False\n",
    "        if not self._has_items(sample):\n",
    "            return False\n",
    "        return True\n",
    "        \n",
    "    def valid_for_reuse(self, sample):\n",
    "        if not self.allowed(sample, do_raise=False):\n",
    "            return False\n",
    "        if not self.is_trashed(sample):\n",
    "            return False\n",
    "#         if self._is_used_in_field_value(sample):\n",
    "#             return False\n",
    "        if self._has_items(sample):\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def reuse(self, sample_type_id):\n",
    "        for s in self.trash:\n",
    "            if s.sample_type_id == sample_type_id and s.id not in self._reused:\n",
    "                if self.valid_for_reuse(s):\n",
    "                    self.clear_field_values(s)\n",
    "                    return s\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sessions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-222a9256f9d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'default'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'aquarium'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrecycler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSampleRecycler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecycler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrash\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sessions' is not defined"
     ]
    }
   ],
   "source": [
    "session = sessions['default']['aquarium']\n",
    "\n",
    "recycler = SampleRecycler(session)\n",
    "\n",
    "for s in recycler.trash:\n",
    "    print(s.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "import hashlib\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import json\n",
    "\n",
    "registry = sessions['default']['registry']\n",
    "session = registry.session\n",
    "\n",
    "PREFIX = \"PlantTF_2020_Campaign_\"\n",
    "primer_type = session.SampleType.find_by_name('Primer')\n",
    "fragment_type = session.SampleType.find_by_name('Fragment')\n",
    "plasmid_type = session.SampleType.find_by_name('Plasmid')\n",
    "recycler = SampleRecycler(session)\n",
    "\n",
    "\n",
    "def seq_sha1(seq: str) -> str:\n",
    "    \"\"\"Convert sequence string into a hash\"\"\"\n",
    "    return hashlib.sha1(seq.strip().upper().encode()).hexdigest()\n",
    "\n",
    "def new_name(molecule):\n",
    "    name = molecule['__name__']\n",
    "    if name == \"PRIMER\":\n",
    "        typename = 'Primer'\n",
    "    elif 'PCR' in name:\n",
    "        typename = 'Fragment'\n",
    "    elif name in ['GAP', 'SHARED_SYNTHESIZED_FRAGMENT']:\n",
    "        typename = 'Synthesized'\n",
    "    else:\n",
    "        raise ValueError\n",
    "    \n",
    "    seqhash = seq_sha1(molecule['sequence']['bases'])\n",
    "    return '{}_{}_{}'.format(typename, PREFIX, seqhash[-8:])\n",
    "\n",
    "def new_sample(sample_type, name, description, project, properties):\n",
    "    new_sample = sample_type.new_sample(\n",
    "        name=name,\n",
    "        description=description,\n",
    "        project=project,\n",
    "        properties=properties\n",
    "    )\n",
    "#     sample = recycler.reuse(sample_type.id)\n",
    "    sample = None\n",
    "    if sample:\n",
    "        for fv in sample.field_values:\n",
    "            assert fv.value is None\n",
    "            assert fv.child_sample_id is None\n",
    "            assert fv.sample is None\n",
    "        sample.name = new_sample.name\n",
    "        sample.description = new_sample.description\n",
    "        sample.update_properties(new_sample.properties)\n",
    "        sample.needs_update = True\n",
    "    else:\n",
    "        sample = new_sample\n",
    "    return sample\n",
    "\n",
    "def _resolve_primer(m):\n",
    "    row = df[df['sequence_hash'] == seq_sha1(m['sequence']['bases'])]\n",
    "    if len(row):\n",
    "        sample_id = int(row['sample_id'].values[0])\n",
    "        \n",
    "        assert sample_id\n",
    "        sample = session.Sample.find(sample_id)\n",
    "        assert sample\n",
    "        return sample\n",
    "    else:\n",
    "        return new_sample(sample_type=primer_type,\n",
    "            name=new_name(m),\n",
    "            description='dasi designed',\n",
    "            project='SD2',\n",
    "            properties={\n",
    "                'Anneal Sequence': m['__meta__']['SEQUENCE'],\n",
    "                'Overhang Sequence': m['__meta__']['OVERHANG'],\n",
    "                'T Anneal': round(m['__meta__']['TM'] - 2, 2)\n",
    "            }\n",
    "        )\n",
    "    \n",
    "def _resolve_fragment(m):\n",
    "    reactions = m['used_as_output_to_reactions']\n",
    "    assert len(reactions) == 1\n",
    "    inputs = results['reactions'][reactions[0]]['inputs']\n",
    "    input_molecules = [results['molecules'][i] for i in inputs]\n",
    "    primers = [m for m in input_molecules if m['__name__'] == 'PRIMER']\n",
    "    templates = [m for m in input_molecules if m['__name__'] == 'TEMPLATE']\n",
    "    assert len(primers) == 2\n",
    "    assert len(templates) == 1\n",
    "    \n",
    "    fwd = _resolve_primer(primers[0])\n",
    "    rev = _resolve_primer(primers[1])\n",
    "    \n",
    "    return new_sample(\n",
    "        sample_type=fragment_type,\n",
    "        name=new_name(m),\n",
    "        description='dasi designed',\n",
    "        project='SD2',\n",
    "        properties={\n",
    "            'Forward Primer': fwd,\n",
    "            'Reverse Primer': rev,\n",
    "            'Template': _resolve_template(templates[0]),\n",
    "            'Length': len(m['sequence']['bases']),\n",
    "            'Sequence': '',\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "def _resolve_template(m):\n",
    "    lims_id = m['sequence']['LIMS_ID']\n",
    "    assert lims_id\n",
    "    sample = session.Sample.find(lims_id)\n",
    "    assert sample\n",
    "    return sample\n",
    "\n",
    "def _resolve_gblock(m):\n",
    "    return new_sample(\n",
    "        sample_type=fragment_type,\n",
    "        name=new_name(m),\n",
    "        description='dasi designed',\n",
    "        project='SD2',\n",
    "        properties={\n",
    "            'Sequence': m['sequence']['bases'],\n",
    "            'Length': len(m['sequence']['bases'])\n",
    "        }\n",
    "    )\n",
    "\n",
    "def _resolve_plasmid(m):\n",
    "    return new_sample(\n",
    "        sample_type=plasmid_type,\n",
    "        name=m['sequence']['name'],\n",
    "        description='dasi designed',\n",
    "        project='SD2',\n",
    "        properties={\n",
    "            'Sequence': m['sequence']['bases'],\n",
    "            'Length': len(m['sequence']['bases']),\n",
    "            'Bacterial Marker': 'Amp'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "def _resolve_molecule(m):\n",
    "    if m['__name__'] in ['GAP', 'SHARED_SYNTHESIZED_FRAGMENT']:\n",
    "        return _resolve_gblock(m)\n",
    "    elif m['__name__'] in ['PCR_PRODUCT', 'PCR_PRODUCT_WITH_LEFT_PRIMER',\n",
    "                          'PCR_PRODUCT_WITH_RIGHT_PRIMER', 'PCR_PRODUCT_WITH_PRIMERS']:\n",
    "        return _resolve_fragment(m)\n",
    "    elif m['__name__'] == 'TEMPLATE':\n",
    "        return _resolve_template(m)\n",
    "    elif m['__name__'] == 'PRE-MADE DNA FRAGMENT':\n",
    "        return _resolve_template(m)\n",
    "    elif m['__name__'] == 'PRIMER':\n",
    "        return _resolve_primer(m)\n",
    "    elif m['__name__'] == 'PLASMID':\n",
    "        return _resolve_plasmid(m)\n",
    "    else:\n",
    "        raise ValueError(m['__name__'] + \" not recognized\")\n",
    "\n",
    "def resolve_molecules(molecules):\n",
    "    for m in molecules:\n",
    "        m['__sample__'] = None\n",
    "\n",
    "    for m in molecules:\n",
    "        resolved = _resolve_molecule(m)\n",
    "        if resolved:\n",
    "            m['__sample__'] = resolved\n",
    "    return molecules\n",
    "\n",
    "##################\n",
    "# Submit\n",
    "##################\n",
    "\n",
    "from pydent import Planner\n",
    "from pydent import save_inventory\n",
    "\n",
    "results = design.out(elim_extra_reactions=True)\n",
    "resolve_molecules(results['molecules'])\n",
    "design_id = seq_sha1(json.dumps(results['designs']))\n",
    "\n",
    "##################\n",
    "# Save inventory\n",
    "##################\n",
    "all_samples = [m['__sample__'] for m in results['molecules']]\n",
    "new_samples = [m['__sample__'] for m in results['molecules'] if not m['__sample__'].id]\n",
    "\n",
    "##################\n",
    "# Update samples\n",
    "##################\n",
    "to_update = [m['__sample__'] for m in results['molecules'] if hasattr(m['__sample__'], 'needs_update')]\n",
    "\n",
    "print(len(new_samples))\n",
    "print(len(to_update))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Graph Plotter\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def sorted_center(iterable, reverse=False, key=None, select=None):\n",
    "    centered = []\n",
    "    s = sorted(iterable, key=key, reverse=reverse)\n",
    "    front = False\n",
    "    for v in s:\n",
    "        if not front:\n",
    "            centered.append(v)\n",
    "        else:\n",
    "            centered.insert(0, v)\n",
    "        front = not front\n",
    "    if select:\n",
    "        return [select(_x) for _x in centered]\n",
    "    return centered\n",
    "\n",
    "class GraphPlotter(object):\n",
    "  \n",
    "  def __init__(self, graph, ax=None, pos=None):\n",
    "    if ax is None:\n",
    "      ax = plt.figure(figsize=(3,3)).gca()\n",
    "      ax.axis('off')\n",
    "      ax.set_xlim(0, 1.0)\n",
    "      ax.set_ylim(0, 1.0)\n",
    "    \n",
    "    self._ax = ax\n",
    "    self._graph = graph\n",
    "    if pos is None:\n",
    "      self._pos = dict()\n",
    "      self.spring_layout()\n",
    "    else:\n",
    "      self._pos = pos\n",
    "      \n",
    "  @property\n",
    "  def _base_draw_kwargs(self):\n",
    "    return dict(G=self._graph, ax=self._ax, pos=self._pos)\n",
    " \n",
    "  def topological_sort(self):\n",
    "    xlim = self._ax.get_xlim()\n",
    "    ylim = self._ax.get_ylim()\n",
    "    \n",
    "    x = xlim[1] - xlim[0]\n",
    "    y = ylim[1] - ylim[0]\n",
    "    xspacer = x * 0.05\n",
    "    yspacer = y * 0.05\n",
    "    \n",
    "    new_xlim = (xlim[0] + xspacer, xlim[1] - xspacer)\n",
    "    new_ylim = (ylim[0] + yspacer, ylim[1] - yspacer)\n",
    "    pos = self._topological_sort(self._graph, \n",
    "                                 xlim=new_xlim, \n",
    "                                 ylim=new_ylim)\n",
    "    self._pos = pos\n",
    "\n",
    "  @staticmethod\n",
    "  def _roots_and_leaves(G, include_cycles=False):\n",
    "    roots = [k for k, v in G.in_degree(G.nodes) if v == 0]\n",
    "    leaves = [k for k, v in G.out_degree(G.nodes) if v == 0]\n",
    "    \n",
    "    if include_cycles:\n",
    "      for c in nx.simple_cycles(G):\n",
    "        outdegree = list(G.out_degree(c))\n",
    "        outdegree.sort(key=lambda x: x[1])\n",
    "        leaves.append(outdegree[0][0])\n",
    "\n",
    "        indegree = list(G.out_degree(c))\n",
    "        indegree.sort(key=lambda x: x[1])\n",
    "        roots.append(indegree[0][0])\n",
    "    return roots, leaves\n",
    "  \n",
    "  @classmethod\n",
    "  def _get_roots(cls, G, include_cycles=False):\n",
    "    return cls._roots_and_leaves(G, include_cycles=include_cycles)[0]\n",
    "\n",
    "  @staticmethod\n",
    "  def _get_leaves(G, include_cycles=False):\n",
    "    return cls._roots_and_leaves(G, include_cycles=include_cycles)[1]\n",
    "\n",
    "  @staticmethod\n",
    "  def _get_subgraphs(graph):\n",
    "    \"\"\"Get independent subgraphs\"\"\"\n",
    "    node_list = list(graph.nodes)\n",
    "    subgraphs = []\n",
    "    while len(node_list) > 0:\n",
    "        node = node_list[-1]\n",
    "        subgraph = nx.bfs_tree(to_undirected(graph), node)\n",
    "        for n in subgraph.nodes:\n",
    "            node_list.remove(n)\n",
    "        subgraphs.append(graph.subgraph(subgraph.nodes))\n",
    "    return subgraphs\n",
    "\n",
    "  @classmethod\n",
    "  def _topological_sort(cls, G, xlim=None, ylim=None):\n",
    "    if xlim is None:\n",
    "        xlim = [0.05, 0.95]\n",
    "    if ylim is None:\n",
    "        ylim = [0.05, 0.95]\n",
    "    \n",
    "    rts = cls._get_roots(G, include_cycles=True)\n",
    "    max_depth = {}\n",
    "    for root in rts:\n",
    "        depths = nx.single_source_shortest_path_length(G, root)\n",
    "        for n, d in depths.items():\n",
    "            max_depth[n] = max(max_depth.get(n, d), d)\n",
    "\n",
    "    by_depth = collections.OrderedDict()\n",
    "\n",
    "    for node, depth in max_depth.items():\n",
    "        by_depth.setdefault(depth, [])\n",
    "        by_depth[depth].append(node)\n",
    "    \n",
    "    # center nodes with highest degree\n",
    "    for depth, nodes in by_depth.items():\n",
    "        centered = sorted_center(list(G.degree(nodes)), \n",
    "                                 key=lambda x: x[1], \n",
    "                                 reverse=True,\n",
    "                                select=lambda x: x[0])\n",
    "        by_depth[depth] = centered\n",
    "\n",
    "    # push roots 'up' so they are not stuck on layer one\n",
    "    for root in rts:\n",
    "        successors = list(G.successors(root))\n",
    "        if len(successors) > 0:\n",
    "            min_depth = min([max_depth[s] for s in successors])\n",
    "            max_depth[root] = min_depth - 1\n",
    "\n",
    "    # assign positions\n",
    "\n",
    "    y_min_max = xlim\n",
    "    x_min_max = ylim\n",
    "    max_width = max([len(layer) for layer in by_depth.values()])\n",
    "\n",
    "    y_step = (y_min_max[1] - y_min_max[0]) / (max(by_depth.keys())+1)\n",
    "    x_step = (x_min_max[1] - x_min_max[0]) / (max_width )\n",
    "    positions = {}\n",
    "\n",
    "    for depth in sorted(by_depth):\n",
    "        y = y_step * depth + y_min_max[0]\n",
    "        node_ids = by_depth[depth]\n",
    "        w = len(node_ids)\n",
    "        delta_w = max_width - w\n",
    "        for i, n in enumerate(node_ids):\n",
    "            x_offset = delta_w * x_step / 2.0\n",
    "            x = x_min_max[0] + x_offset + i * x_step\n",
    "            positions[n] = (x, y)\n",
    "    return positions\n",
    "\n",
    "  def spring_layout(self, **kwargs):\n",
    "    pos = nx.spring_layout(self._graph, **kwargs)\n",
    "    self._pos.update(pos)\n",
    "    return self._pos\n",
    "    \n",
    "  @property\n",
    "  def nodes(self):\n",
    "    return self._graph.nodes\n",
    "  \n",
    "  @property\n",
    "  def edges(self):\n",
    "    return self._graph.edges\n",
    "  \n",
    "  def node_attrs(self):\n",
    "    keys = set()\n",
    "    for _, node_data in self._graph.nodes(data=True):\n",
    "      keys.update(set(node_data.keys()))\n",
    "    attrs = {}\n",
    "    for _, node_data in self._graph.nodes(data=True):\n",
    "      for k in keys:\n",
    "        attrs.setdefault(k, list())\n",
    "        attrs[k].append(node_data.get(k, None))\n",
    "    return attrs\n",
    "  \n",
    "  def edge_attrs(self):\n",
    "    keys = set()\n",
    "    for _, _, edge_data in self._graph.edges(data=True):\n",
    "      keys.update(set(edge_data.keys()))\n",
    "    attrs = {}\n",
    "    for _, _, edge_data in self._graph.edges(data=True):\n",
    "      for k in keys:\n",
    "        attrs.setdefault(k, list())\n",
    "        attrs[k].append(edge_data.get(k, None))\n",
    "    return attrs\n",
    "  \n",
    "  def map_edge_attrs(self, attrs, source, target):\n",
    "    vals = [self._normalize(self.edge_attrs()[attr], source, target) for attr in attrs]\n",
    "    return dict(zip(attrs, vals))\n",
    "  \n",
    "  def map_node_attrs(self, attrs, source, target):\n",
    "    vals = [self._normalize(self.node_attrs()[attr], source, target) for attr in attrs]\n",
    "    return dict(zip(attrs, vals))\n",
    "  \n",
    "  def _normalize(arr, source, target):\n",
    "    x = source[1] - source[0]\n",
    "    y = target[1] - target[0]\n",
    "    return [_x/x * y + target[0] for _x in arr] \n",
    "  \n",
    "  def _make_draw_kwargs(self, **kwargs):\n",
    "    kwargs.update(self._base_draw_kwargs)\n",
    "    return kwargs\n",
    "  \n",
    "  def _draw(self, draw_function, zorder=None, **kwargs):\n",
    "    draw_kwargs = self._make_draw_kwargs(**kwargs)\n",
    "    collection = draw_function(**draw_kwargs)\n",
    "    if collection is not None and zorder is not None:\n",
    "      try:\n",
    "        # This is for compatibility with older matplotlib.\n",
    "        collection.set_zorder(zorder)\n",
    "      except AttributeError:\n",
    "        # This is for compatibility with newer matplotlib.\n",
    "        collection[0].set_zorder(zorder)\n",
    "    return collection\n",
    "\n",
    "  def draw_nodes(self, **kwargs):\n",
    "    \"\"\"Useful kwargs: nodelist, node_size, node_color, linewidths.\"\"\"\n",
    "    if (\"node_color\" in kwargs and\n",
    "        isinstance(kwargs[\"node_color\"], collections.Sequence) and\n",
    "        len(kwargs[\"node_color\"]) in {3, 4} and\n",
    "        not isinstance(kwargs[\"node_color\"][0],\n",
    "                       (collections.Sequence, np.ndarray))):\n",
    "      num_nodes = len(kwargs.get(\"nodelist\", self.nodes))\n",
    "      kwargs[\"node_color\"] = np.tile(\n",
    "          np.array(kwargs[\"node_color\"])[None], [num_nodes, 1])\n",
    "    return self._draw(nx.draw_networkx_nodes, **kwargs)\n",
    "\n",
    "  def draw_edges(self, **kwargs):\n",
    "    \"\"\"Useful kwargs: edgelist, width.\"\"\"\n",
    "    return self._draw(nx.draw_networkx_edges, **kwargs)\n",
    "  \n",
    "  def draw_graph(self,\n",
    "                 node_size=200,\n",
    "                 node_color=(0.4, 0.8, 0.4),\n",
    "                 node_linewidth=1.0,\n",
    "                 edge_width=1.0):\n",
    "    \n",
    "    node_border_color = (0.0, 0.0, 0.0, 1.0)\n",
    "    \n",
    "    # Plot nodes.\n",
    "    self.draw_nodes(\n",
    "        nodelist=self.nodes,\n",
    "        node_size=node_size,\n",
    "        node_color=node_color,\n",
    "        linewidths=node_linewidth,\n",
    "        edgecolors=node_border_color,\n",
    "        zorder=20)\n",
    "    # Plot edges.\n",
    "    self.draw_edges(edgelist=self.edges, width=edge_width, zorder=10)\n",
    "\n",
    "  \n",
    "  \n",
    "g = nx.balanced_tree(2, 2)\n",
    "g = nx.to_directed(g)\n",
    "\n",
    "for e in g.edges:\n",
    "  g.edges[e[0], e[1]]['weight'] = 1\n",
    "  \n",
    "ax = plt.figure(figsize=(3,3)).gca()\n",
    "ax.axis('off')\n",
    "ax.set_xlim(0, 1.0)\n",
    "ax.set_ylim(0, 1.0)\n",
    "    \n",
    "plotter = GraphPlotter(g, ax=ax)\n",
    "plotter.topological_sort()\n",
    "\n",
    "plotter.draw_graph(node_color=(0, 0, 0), edge_width=plotter.edge_attrs()['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydent import Planner\n",
    "import networkx as nx\n",
    "%matplotlib inline\n",
    "\n",
    "g = nx.DiGraph()\n",
    "for qk, d in results['designs'].items():\n",
    "    if not len(d['assemblies']):\n",
    "        print(\"MISSING ASSEMBLY FOR {}\".format(d.keys()))\n",
    "        continue\n",
    "    reaction_id = d['assemblies'][0]['final_assembly_reaction'][0]\n",
    "    \n",
    "    assembly_reaction = results['reactions'][reaction_id]\n",
    "    in_samples = [results['molecules'][i]['__sample__'] for i in assembly_reaction['inputs']]\n",
    "    out_samples = [results['molecules'][i]['__sample__'] for i in assembly_reaction['outputs']]\n",
    "    print(out_samples[0].name)\n",
    "    for s in in_samples:\n",
    "        print('\\t{}'.format(s.name))\n",
    "        \n",
    "        g.add_edge(s.name, out_samples[0].name)\n",
    "\n",
    "plotter = GraphPlotter(g)\n",
    "plotter.topological_sort()\n",
    "\n",
    "plotter.draw_graph()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if updating samples exist\n",
    "\n",
    "exists = []\n",
    "for u in to_update:\n",
    "    print(u.id)\n",
    "    existing = session.Sample.find_by_name(u.name)\n",
    "    if existing:\n",
    "        print('{} exists'.format(u.name))\n",
    "        exists.append(existing)\n",
    "        \n",
    "if exists:\n",
    "    raise ValueError\n",
    "#     assert not session.Sample.find_by_name(u.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
