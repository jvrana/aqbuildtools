{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DASi to Aquarium\n",
    "\n",
    "**Inputs**\n",
    "\n",
    "* `design.out.json`\n",
    "* `SelectedInventory.csv`\n",
    "\n",
    "**Outputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'default': {'aquarium': <AqSession(name=None, AqHTTP=<AqHTTP(user='vrana', url='http://52.27.43.242')>), parent=94039747511136)>,\n",
       "  'benchling': <benchlingapi.session.Session at 0x7f99b0d71c10>,\n",
       "  'registry': <aqbt.aquarium.registry.LabDNARegistry at 0x7f99b0c04670>}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# login to Benchling & Aquarium\n",
    "from aqbt.tools import config_to_sessions\n",
    "from aqbt.tools import parse_config\n",
    "import toml\n",
    "\n",
    "def config(config_path):\n",
    "    with open(config_path, \"r\") as f:\n",
    "        return parse_config(toml.load(f))\n",
    "\n",
    "\n",
    "def sessions(config):\n",
    "    return config_to_sessions(config)\n",
    "\n",
    "\n",
    "sessions = sessions(config('config.toml'))\n",
    "\n",
    "benchling = sessions['default']['benchling']\n",
    "registry = sessions['default']['registry']\n",
    "session = sessions['default']['aquarium']\n",
    "sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load `SelectedInventory.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_registry_id</th>\n",
       "      <th>is_available</th>\n",
       "      <th>is_circular</th>\n",
       "      <th>record</th>\n",
       "      <th>record_uuid</th>\n",
       "      <th>sample</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>sample_type</th>\n",
       "      <th>sequence_hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>ID: 1010__vf2\\nName: vf2\\nDescription: &lt;unknow...</td>\n",
       "      <td>b227364f-4708-4976-a960-34e8fb019670</td>\n",
       "      <td>&lt;Sample id=1010 name=vf2 sample_type={'rid': 1...</td>\n",
       "      <td>1010</td>\n",
       "      <td>Primer</td>\n",
       "      <td>ed119701ab23e88921ecee8ef629a110265c65ae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>ID: 1014__c-myc-F\\nName: c-myc-f\\nDescription:...</td>\n",
       "      <td>e27abdee-dda1-4ede-8975-2c2f43d631f8</td>\n",
       "      <td>&lt;Sample id=1014 name=c-myc-F sample_type={'rid...</td>\n",
       "      <td>1014</td>\n",
       "      <td>Primer</td>\n",
       "      <td>d48a98059541b2acf77b13bf59645a5f3cdc5ca5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>ID: 1015__trp_F\\nName: trp_f\\nDescription: &lt;un...</td>\n",
       "      <td>365e36ea-ec97-4e5f-88a5-3a21ae2e8077</td>\n",
       "      <td>&lt;Sample id=1015 name=trp_F sample_type={'rid':...</td>\n",
       "      <td>1015</td>\n",
       "      <td>Primer</td>\n",
       "      <td>78dd1501c912b9f080e72e7854fa6385e01d0757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>ID: 1016__cen6_F\\nName: cen6_f\\nDescription: &lt;...</td>\n",
       "      <td>df022340-6045-4368-9e20-4daa01642695</td>\n",
       "      <td>&lt;Sample id=1016 name=cen6_F sample_type={'rid'...</td>\n",
       "      <td>1016</td>\n",
       "      <td>Primer</td>\n",
       "      <td>323188271e4745f3d111680404159569e643890b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>ID: 1039__PP2-Gal1m-f\\nName: pp2-gal1m-f\\nDesc...</td>\n",
       "      <td>fb3ff6dc-edf5-40dd-8171-83f3ac2d1d3b</td>\n",
       "      <td>&lt;Sample id=1039 name=PP2-Gal1m-f sample_type={...</td>\n",
       "      <td>1039</td>\n",
       "      <td>Primer</td>\n",
       "      <td>bb336cb902b7593df17ca1ee42113f3a54b75912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    entity_registry_id  is_available  is_circular  \\\n",
       "286                NaN          True        False   \n",
       "287                NaN          True        False   \n",
       "288                NaN          True        False   \n",
       "289                NaN          True        False   \n",
       "290                NaN          True        False   \n",
       "\n",
       "                                                record  \\\n",
       "286  ID: 1010__vf2\\nName: vf2\\nDescription: <unknow...   \n",
       "287  ID: 1014__c-myc-F\\nName: c-myc-f\\nDescription:...   \n",
       "288  ID: 1015__trp_F\\nName: trp_f\\nDescription: <un...   \n",
       "289  ID: 1016__cen6_F\\nName: cen6_f\\nDescription: <...   \n",
       "290  ID: 1039__PP2-Gal1m-f\\nName: pp2-gal1m-f\\nDesc...   \n",
       "\n",
       "                              record_uuid  \\\n",
       "286  b227364f-4708-4976-a960-34e8fb019670   \n",
       "287  e27abdee-dda1-4ede-8975-2c2f43d631f8   \n",
       "288  365e36ea-ec97-4e5f-88a5-3a21ae2e8077   \n",
       "289  df022340-6045-4368-9e20-4daa01642695   \n",
       "290  fb3ff6dc-edf5-40dd-8171-83f3ac2d1d3b   \n",
       "\n",
       "                                                sample  sample_id sample_type  \\\n",
       "286  <Sample id=1010 name=vf2 sample_type={'rid': 1...       1010      Primer   \n",
       "287  <Sample id=1014 name=c-myc-F sample_type={'rid...       1014      Primer   \n",
       "288  <Sample id=1015 name=trp_F sample_type={'rid':...       1015      Primer   \n",
       "289  <Sample id=1016 name=cen6_F sample_type={'rid'...       1016      Primer   \n",
       "290  <Sample id=1039 name=PP2-Gal1m-f sample_type={...       1039      Primer   \n",
       "\n",
       "                                sequence_hash  \n",
       "286  ed119701ab23e88921ecee8ef629a110265c65ae  \n",
       "287  d48a98059541b2acf77b13bf59645a5f3cdc5ca5  \n",
       "288  78dd1501c912b9f080e72e7854fa6385e01d0757  \n",
       "289  323188271e4745f3d111680404159569e643890b  \n",
       "290  bb336cb902b7593df17ca1ee42113f3a54b75912  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_df = pd.read_csv(\"SelectedInventory.csv\", index_col=0)\n",
    "inv_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load `design.out.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['metadata', 'designs', 'molecules', 'reactions'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'program': 'dasi',\n",
       " 'version': '0.0.21',\n",
       " 'execution_trace': {'compile': {'__name__': 'compile',\n",
       "   '__spec__': \"FullArgSpec(args=['self', 'n_jobs', 'post_processing_kwargs'], varargs=None, varkw=None, defaults=(10, None), kwonlyargs=[], kwonlydefaults=None, annotations={'n_jobs': <class 'int'>, 'post_processing_kwargs': typing.Dict})\",\n",
       "   'start': '2020-07-21 07:03:44.579428',\n",
       "   'end': '2020-07-21 07:06:49.781327',\n",
       "   'args': {'n_jobs': 6, 'post_processing_kwargs': None},\n",
       "   'algorithm': 'library optimization'},\n",
       "  'optimize': {'__name__': 'optimize',\n",
       "   '__spec__': \"FullArgSpec(args=['self', 'n_paths', 'n_jobs'], varargs=None, varkw=None, defaults=(3, 10), kwonlyargs=[], kwonlydefaults=None, annotations={'return': typing.Dict[str, dasi.design.design.DesignResult], 'n_paths': <class 'int'>, 'n_jobs': <class 'int'>})\",\n",
       "   'start': '2020-07-21 07:06:49.781447',\n",
       "   'end': '2020-07-21 07:07:29.300317',\n",
       "   'args': {'n_paths': 1, 'n_jobs': 6},\n",
       "   'algorithm': 'library optimization'}}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('design.out.json', 'r') as f:\n",
    "    design_out = json.load(f)\n",
    "    \n",
    "print(design_out.keys())\n",
    "design_out['metadata']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Output to Aquarium Input Spreadsheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "import hashlib\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import json\n",
    "\n",
    "registry = sessions['default']['registry']\n",
    "session = registry.session\n",
    "\n",
    "PREFIX = \"PlantTF_2020_Campaign_2_\"\n",
    "primer_type = session.SampleType.find_by_name('Primer')\n",
    "fragment_type = session.SampleType.find_by_name('Fragment')\n",
    "plasmid_type = session.SampleType.find_by_name('Plasmid')\n",
    "# recycler = SampleRecycler(session)\n",
    "\n",
    "\n",
    "def seq_sha1(seq: str) -> str:\n",
    "    \"\"\"Convert sequence string into a hash\"\"\"\n",
    "    return hashlib.sha1(seq.strip().upper().encode()).hexdigest()\n",
    "\n",
    "def new_name(molecule):\n",
    "    name = molecule['__name__']\n",
    "    if name == \"PRIMER\":\n",
    "        typename = 'Primer'\n",
    "    elif 'PCR' in name:\n",
    "        typename = 'Fragment'\n",
    "    elif name in ['GAP', 'SHARED_SYNTHESIZED_FRAGMENT']:\n",
    "        typename = 'Synthesized'\n",
    "    else:\n",
    "        raise ValueError\n",
    "    \n",
    "    seqhash = seq_sha1(molecule['sequence']['bases'])\n",
    "    return '{}_{}_{}'.format(typename, PREFIX, seqhash[-8:])\n",
    "\n",
    "def new_sample(sample_type, name, description, project, properties):\n",
    "    new_sample = sample_type.new_sample(\n",
    "        name=name,\n",
    "        description=description,\n",
    "        project=project,\n",
    "        properties=properties\n",
    "    )\n",
    "#     sample = recycler.reuse(sample_type.id)\n",
    "    sample = None\n",
    "    if sample:\n",
    "        for fv in sample.field_values:\n",
    "            assert fv.value is None\n",
    "            assert fv.child_sample_id is None\n",
    "            assert fv.sample is None\n",
    "        sample.name = new_sample.name\n",
    "        sample.description = new_sample.description\n",
    "        sample.update_properties(new_sample.properties)\n",
    "        sample.needs_update = True\n",
    "    else:\n",
    "        sample = new_sample\n",
    "    return sample\n",
    "\n",
    "def _resolve_primer(m, inv_df):\n",
    "    row = inv_df[inv_df['sequence_hash'] == seq_sha1(m['sequence']['bases'])]\n",
    "    if len(row):\n",
    "        sample_id = int(row['sample_id'].values[0])\n",
    "        \n",
    "        assert sample_id\n",
    "        sample = session.Sample.find(sample_id)\n",
    "        assert sample\n",
    "        return sample\n",
    "    else:\n",
    "        return new_sample(sample_type=primer_type,\n",
    "            name=new_name(m),\n",
    "            description='dasi designed',\n",
    "            project='SD2',\n",
    "            properties={\n",
    "                'Anneal Sequence': m['__meta__']['SEQUENCE'],\n",
    "                'Overhang Sequence': m['__meta__']['OVERHANG'],\n",
    "                'T Anneal': round(m['__meta__']['TM'] - 2, 2)\n",
    "            }\n",
    "        )\n",
    "    \n",
    "def _resolve_fragment(m, results, df):\n",
    "    reactions = m['used_as_output_to_reactions']\n",
    "    assert len(reactions) == 1\n",
    "    inputs = results['reactions'][reactions[0]]['inputs']\n",
    "    input_molecules = [results['molecules'][i] for i in inputs]\n",
    "    primers = [m for m in input_molecules if m['__name__'] == 'PRIMER']\n",
    "    templates = [m for m in input_molecules if m['__name__'] == 'TEMPLATE']\n",
    "    assert len(primers) == 2\n",
    "    assert len(templates) == 1\n",
    "    \n",
    "    fwd = _resolve_primer(primers[0], df)\n",
    "    rev = _resolve_primer(primers[1],df)\n",
    "    \n",
    "    return new_sample(\n",
    "        sample_type=fragment_type,\n",
    "        name=new_name(m),\n",
    "        description='dasi designed',\n",
    "        project='SD2',\n",
    "        properties={\n",
    "            'Forward Primer': fwd,\n",
    "            'Reverse Primer': rev,\n",
    "            'Template': _resolve_template(templates[0]),\n",
    "            'Length': len(m['sequence']['bases']),\n",
    "            'Sequence': '',\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "def _resolve_template(m):\n",
    "    lims_id = m['sequence']['LIMS_ID']\n",
    "    assert lims_id\n",
    "    sample = session.Sample.find(lims_id)\n",
    "    assert sample\n",
    "    return sample\n",
    "\n",
    "def _resolve_gblock(m):\n",
    "    return new_sample(\n",
    "        sample_type=fragment_type,\n",
    "        name=new_name(m),\n",
    "        description='dasi designed',\n",
    "        project='SD2',\n",
    "        properties={\n",
    "            'Sequence': m['sequence']['bases'],\n",
    "            'Length': len(m['sequence']['bases'])\n",
    "        }\n",
    "    )\n",
    "\n",
    "def _resolve_plasmid(m):\n",
    "    return new_sample(\n",
    "        sample_type=plasmid_type,\n",
    "        name=m['sequence']['name'],\n",
    "        description='dasi designed',\n",
    "        project='SD2',\n",
    "        properties={\n",
    "            'Sequence': m['sequence']['bases'],\n",
    "            'Length': len(m['sequence']['bases']),\n",
    "            'Bacterial Marker': 'Amp'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "def _resolve_molecule(m, results, df):\n",
    "    if m['__name__'] in ['GAP', 'SHARED_SYNTHESIZED_FRAGMENT']:\n",
    "        return _resolve_gblock(m)\n",
    "    elif m['__name__'] in ['PCR_PRODUCT', 'PCR_PRODUCT_WITH_LEFT_PRIMER',\n",
    "                          'PCR_PRODUCT_WITH_RIGHT_PRIMER', 'PCR_PRODUCT_WITH_PRIMERS']:\n",
    "        return _resolve_fragment(m, results, df)\n",
    "    elif m['__name__'] == 'TEMPLATE':\n",
    "        return _resolve_template(m)\n",
    "    elif m['__name__'] == 'PRE-MADE DNA FRAGMENT':\n",
    "        return _resolve_template(m)\n",
    "    elif m['__name__'] == 'PRIMER':\n",
    "        return _resolve_primer(m, df)\n",
    "    elif m['__name__'] == 'PLASMID':\n",
    "        return _resolve_plasmid(m)\n",
    "    else:\n",
    "        raise ValueError(m['__name__'] + \" not recognized\")\n",
    "\n",
    "def resolve_molecules(results, df):\n",
    "    molecules = results['molecules']\n",
    "    for m in molecules:\n",
    "        m['__sample__'] = None\n",
    "\n",
    "    for m in molecules:\n",
    "        resolved = _resolve_molecule(m, results, df)\n",
    "        if resolved:\n",
    "            m['__sample__'] = resolved\n",
    "    return molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# Submit\n",
    "##################\n",
    "\n",
    "from pydent import Planner\n",
    "from pydent import save_inventory\n",
    "\n",
    "resolve_molecules(design_out, inv_df)\n",
    "design_id = seq_sha1(json.dumps(design_out['designs']))\n",
    "\n",
    "##################\n",
    "# Save inventory\n",
    "##################\n",
    "all_samples = [m['__sample__'] for m in design_out['molecules']]\n",
    "new_samples = [m['__sample__'] for m in design_out['molecules'] if not m['__sample__'].id]\n",
    "\n",
    "##################\n",
    "# Update samples\n",
    "##################\n",
    "to_update = [m['__sample__'] for m in design_out['molecules'] if hasattr(m['__sample__'], 'needs_update')]\n",
    "\n",
    "print(len(new_samples))\n",
    "print(len(to_update))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'program': 'dasi',\n",
       " 'version': '0.0.21',\n",
       " 'execution_trace': {'compile': {'__name__': 'compile',\n",
       "   '__spec__': \"FullArgSpec(args=['self', 'n_jobs', 'post_processing_kwargs'], varargs=None, varkw=None, defaults=(10, None), kwonlyargs=[], kwonlydefaults=None, annotations={'n_jobs': <class 'int'>, 'post_processing_kwargs': typing.Dict})\",\n",
       "   'start': '2020-07-21 07:03:44.579428',\n",
       "   'end': '2020-07-21 07:06:49.781327',\n",
       "   'args': {'n_jobs': 6, 'post_processing_kwargs': None},\n",
       "   'algorithm': 'library optimization'},\n",
       "  'optimize': {'__name__': 'optimize',\n",
       "   '__spec__': \"FullArgSpec(args=['self', 'n_paths', 'n_jobs'], varargs=None, varkw=None, defaults=(3, 10), kwonlyargs=[], kwonlydefaults=None, annotations={'return': typing.Dict[str, dasi.design.design.DesignResult], 'n_paths': <class 'int'>, 'n_jobs': <class 'int'>})\",\n",
       "   'start': '2020-07-21 07:06:49.781447',\n",
       "   'end': '2020-07-21 07:07:29.300317',\n",
       "   'args': {'n_paths': 1, 'n_jobs': 6},\n",
       "   'algorithm': 'library optimization'}}}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "design_out['metadata']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lint Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aqbt.aquarium import Linter\n",
    "linter = Linter()\n",
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spreadsheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Validating samples **********\n",
      "Sample validated: 34866:pMOD6_backbone_Input_node_be_jub1\n",
      "Sample validated: 34867:pMOD6_backbone_Input_node_be_ataf1\n",
      "Sample validated: 34868:pMOD6_backbone_Input_node_be_ant\n",
      "Sample validated: 34869:pMOD6_backbone_Input_node_be_anac102\n",
      "Sample validated: 34871:pMOD6_backbone_Input_node_doxy_jub1\n",
      "Sample validated: 34873:pMOD6_backbone_Input_node_doxy_ataf1\n",
      "Sample validated: 34874:pMOD6_backbone_Input_node_doxy_anac102\n",
      "Sample validated: 34898:pMOD-HO-KanMX_output_node_4b2_anac102_stable_gfp\n",
      "Sample validated: 34901:pMOD-HO-KanMX_output_node_6b2_ataf1_stable_gfp\n",
      "Sample validated: 34902:pMOD-HO-KanMX_output_node_2b2_ant_stable_gfp\n",
      "Sample validated: 34903:pMOD-HO-KanMX_output_node_6b2_anac102_stable_gfp\n",
      "Sample validated: 34904:pMOD-HO-KanMX_output_node_6b2_jub1_stable_gfp\n",
      "Sample validated: 34905:pMOD-HO-KanMX_output_node_6b2_ant_stable_gfp\n",
      "Sample validated: 34906:pMOD-HO-KanMX_output_node_2b2_ataf1_stable_gfp\n",
      "Sample validated: 34908:pMOD-HO-KanMX_output_node_4b2_ataf1_stable_gfp\n",
      "Sample validated: 34909:pMOD-HO-KanMX_output_node_2b2_anac102_stable_gfp\n",
      "Sample validated: 34910:pMOD-HO-KanMX_output_node_2b2_jub1_stable_gfp\n",
      "Sample validated: 34912:pMOD-HO-KanMX_output_node_4b2_ant_stable_gfp\n",
      "Sample validated: 34907:pMOD-HO-KanMX_output_node_4b2_jub1_stable_gfp\n",
      "Sample validated: 34878:pMOD6_backbone_Input_node_doxy_ant\n",
      "Sample validated: 34880:pMOD8_Backbone_medium_exp_strong_ad_tet\n",
      "Sample validated: 34881:pMOD8_Backbone_medium_exp_strong_ad_zev\n",
      "Sample validated: 34882:pMOD8_Backbone_medium_exp_med_ad_zev\n",
      "Sample validated: 34888:pMOD8_Backbone_strong_exp_strong_ad_tet\n",
      "Sample validated: 34889:pMOD8_Backbone_weak_exp_strong_ad_tet\n",
      "Sample validated: 34890:pMOD8_Backbone_strong_exp_strong_ad_zev\n",
      "Sample validated: 34891:pMOD8_Backbone_weak_exp_strong_ad_zev\n",
      "Sample validated: 34893:pMOD8_Backbone_weak_exp_med_ad_zev\n",
      "Sample validated: 34876:pMOD8_Backbone_weak_exp_med_ad_tet\n",
      "73 samples need saving\n",
      "0 samples need updating\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from pydent.models import Sample\n",
    "import dictdiffer\n",
    "\n",
    "def samples_to_dfs(session, samples: List[Sample]):\n",
    "    sample_by_st = {}\n",
    "    with session.with_cache(timeout=60) as sess:\n",
    "        sess.browser.get(samples, {\n",
    "            'sample_type': {\n",
    "                'field_types': 'allowable_field_types'\n",
    "            },\n",
    "            'field_values': {\n",
    "                'sample': 'sample_type',\n",
    "                'field_type': [],\n",
    "                'allowable_field_type': []\n",
    "            }\n",
    "        })\n",
    "    for s in samples:\n",
    "        sample_by_st.setdefault(s.sample_type.name, {\n",
    "            'samples': [],\n",
    "            'sample_type': s.sample_type\n",
    "        })\n",
    "        sample_by_st[s.sample_type.name]['samples'].append(s)\n",
    "        \n",
    "    data = {}\n",
    "    for k, v in sample_by_st.items():\n",
    "        data[k] = _samples_to_df(v['samples'], v['sample_type'], 'idk')\n",
    "    return data\n",
    "\n",
    "def _resolve_field_value(fv):\n",
    "    if fv.ftype == 'sample':\n",
    "        if fv.sample:\n",
    "            return fv.sample.name\n",
    "        elif fv.child_sample_id:\n",
    "            return fv.child_sample_id\n",
    "    else:\n",
    "        return fv.value\n",
    "    \n",
    "def _samples_to_df(samples, sample_type, project):\n",
    "    prop_columns = []\n",
    "    for ft in sample_type.field_types:\n",
    "        if ft.array:\n",
    "            prop_columns.append([ft.name])\n",
    "        else:\n",
    "            prop_columns.append(ft.name)\n",
    "    columns = [sample_type.name, 'Description', 'Project'] + prop_columns\n",
    "    \n",
    "    rows = []\n",
    "    for s in samples:\n",
    "        row = {\n",
    "            sample_type.name: s.name,\n",
    "            'Description': s.description,\n",
    "            'Project': project\n",
    "        }\n",
    "        for prop in prop_columns:\n",
    "            if isinstance(prop, list):\n",
    "                pass\n",
    "            else:\n",
    "                v = s.properties[prop]\n",
    "                if isinstance(v, Sample):\n",
    "                    v = v.name\n",
    "                row[prop] = v\n",
    "        rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# existing samples\n",
    "with session(using_cache=False) as sess:\n",
    "    names = [s.name for s in new_samples]\n",
    "    existing = sess.query({\n",
    "        '__model__': 'Sample',\n",
    "        '__query__': {\n",
    "            'name': names\n",
    "        }\n",
    "    })\n",
    "    existing_by_name = {s.name: s for s in existing}\n",
    "\n",
    "def props(s):\n",
    "    if not s.field_values:\n",
    "        return {}\n",
    "    data = s.properties\n",
    "    for k, v in data.items():\n",
    "        if v.__class__.__name__ == 'Sample':\n",
    "            data[k] = v.id\n",
    "    return data\n",
    "            \n",
    "    \n",
    "def compare_samples(s1, s2):\n",
    "    d1 = props(s1)\n",
    "    d2 = props(s2)\n",
    "    return list(dictdiffer.diff(d1, d2))\n",
    "\n",
    "    \n",
    "assert compare_samples(new_samples[0], new_samples[1])\n",
    "assert not compare_samples(new_samples[0], new_samples[0])\n",
    "\n",
    "from typing import Tuple, List\n",
    "\n",
    "def update_samples(update_from_left_to_right: List[Tuple[Sample, Sample]]):\n",
    "    \"\"\"Safely update samples, carefully validating server and local Sample properties.\"\"\"\n",
    "    needs_updating = update_from_left_to_right\n",
    "\n",
    "    for s1, s2 in needs_updating:\n",
    "        with session(using_cache=False) as sess:\n",
    "            sample_from_server = sess.Sample.find(s2.id)\n",
    "        diff = compare_samples(s1, sample_from_server)\n",
    "        if not diff:\n",
    "            s1.id = sample_from_server.id\n",
    "            print(\"Does not need update: {}:{}\".format(s1.id, s1.name))\n",
    "        else:\n",
    "            print(\"Updating {}:{}\".format(s2.id, s2.name))\n",
    "            s2.update_properties(s1.properties)\n",
    "            # attempt server update\n",
    "            s2.update()\n",
    "\n",
    "            with session(using_cache=False) as sess:\n",
    "                s3 = sess.Sample.find(s2.id)\n",
    "            diff = compare_samples(s2, s3)\n",
    "            if diff:\n",
    "                raise Exception(\"Update not successful\")\n",
    "\n",
    "needs_updating = []\n",
    "needs_saving = []\n",
    "    \n",
    "# attempt update on existing samples\n",
    "print(\"*\" * 10 + \" Validating samples \" + \"*\" * 10)\n",
    "for s in new_samples:\n",
    "    if s.name in existing_by_name:\n",
    "        s2 = existing_by_name[s.name]\n",
    "        diff = compare_samples(s, s2)\n",
    "        if diff:\n",
    "            needs_updating.append((s, s2))\n",
    "        elif s.id is None:\n",
    "            print('Updating id: {}:{}'.format(s.id, s2.name))\n",
    "            s.id = s2.id\n",
    "        else:\n",
    "            print(\"Sample validated: {}:{}\".format(s.id, s.name))\n",
    "    else:\n",
    "        needs_saving.append(s)\n",
    "            \n",
    "print(\"{} samples need saving\".format(len(needs_saving)))\n",
    "\n",
    "print(\"{} samples need updating\".format(len(needs_updating)))\n",
    "update_samples(needs_updating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** To Upload: ***************\n",
      "Step 1: Upload `Upload_Primer.csv`\n",
      "Step 2: Upload `Upload_Fragment.csv`\n",
      "Step 3: Upload `Upload_Plasmid.csv`\n"
     ]
    }
   ],
   "source": [
    "upload_dfs = samples_to_dfs(session, needs_saving)\n",
    "\n",
    "for k, v in upload_dfs.items():\n",
    "    v.to_csv(\"Upload_{}.csv\".format(k), index=False)\n",
    "    \n",
    "print(\"*\"*15 + ' To Upload: ' + \"*\"*15)\n",
    "print(\"Step 1: Upload `{}`\".format('Upload_Primer.csv'))\n",
    "print(\"Step 2: Upload `{}`\".format('Upload_Fragment.csv'))\n",
    "print(\"Step 3: Upload `{}`\".format('Upload_Plasmid.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.to_csv('temp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Primer</th>\n",
       "      <th>Description</th>\n",
       "      <th>Project</th>\n",
       "      <th>Overhang Sequence</th>\n",
       "      <th>Anneal Sequence</th>\n",
       "      <th>T Anneal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Primer_PlantTF_2020_Campaign__ccf58534</td>\n",
       "      <td>dasi designed</td>\n",
       "      <td>idk</td>\n",
       "      <td>GTCAA</td>\n",
       "      <td>AACATTAGTTATGTCACGCTTACAT</td>\n",
       "      <td>55.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Primer_PlantTF_2020_Campaign__29ab61ba</td>\n",
       "      <td>dasi designed</td>\n",
       "      <td>idk</td>\n",
       "      <td>TCGAGTCA</td>\n",
       "      <td>TGATACCGTCGACCTCGAGT</td>\n",
       "      <td>58.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Primer    Description Project  \\\n",
       "0  Primer_PlantTF_2020_Campaign__ccf58534  dasi designed     idk   \n",
       "1  Primer_PlantTF_2020_Campaign__29ab61ba  dasi designed     idk   \n",
       "\n",
       "  Overhang Sequence            Anneal Sequence  T Anneal  \n",
       "0             GTCAA  AACATTAGTTATGTCACGCTTACAT     55.16  \n",
       "1          TCGAGTCA       TGATACCGTCGACCTCGAGT     58.11  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_dfs['Primer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aquarium Sample Recycler\n",
    "\n",
    "This class recycles samples that have been `trashed` (aka have \"TRASHED\" prefixed in their name).\n",
    "\n",
    "Call `recycler = SampleRecycler(session)` to create a new instance. \n",
    "\n",
    "To grab a trashed sample, call `recycler.reuse(sample_type_id)` to choose a trashed sample. A valid trashed sample\n",
    "should have no items, should not be used in any planned operation as an input, should not be used as a property in any field value for a non-trashed sample, and should have no field value properties.\n",
    "\n",
    "To trash a new sample, `recycler.trash_sample(sample)` which will append \"TRASHED\" prefix to its name and delete all of its field values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleRecycler(object):\n",
    "    \n",
    "    TRASHED = 'TRASHED'\n",
    "    USER_ID = 66\n",
    "    \n",
    "    def __init__(self, session):\n",
    "        self.session = session()\n",
    "        self.session.browser.model_cache = {}\n",
    "        self.session.using_cache = False\n",
    "        self._reused = []\n",
    "        self._trash = None\n",
    "    \n",
    "    @classmethod\n",
    "    def allowed(cls, sample, do_raise: bool = True):\n",
    "        if sample.user_id != cls.USER_ID:\n",
    "            if do_raise:\n",
    "                raise ValueError(\"Opps! Cannot access other users samples!\")\n",
    "            else:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    @classmethod\n",
    "    def is_trashed(cls, sample):\n",
    "        if not cls.allowed(sample, do_raise=False):\n",
    "            return False\n",
    "        elif sample.name.startswith(cls.TRASHED):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def trash_sample(self, sample):\n",
    "        if not self.can_be_trashed(sample):\n",
    "            print(\"Cannot trash sample\")\n",
    "            return\n",
    "        if not self.is_trashed(sample):\n",
    "            sample.name = \"TRASHED__\" + sample.name\n",
    "            sample.description = json.dumps({\n",
    "                'is_trashed': True,\n",
    "                'trashed_on': str(arrow.now()),\n",
    "                'previous_name': sample.name\n",
    "            })\n",
    "            \n",
    "            self.clear_field_values(sample)\n",
    "            self.trash.append(sample)\n",
    "            try:\n",
    "                sample.save()\n",
    "            except:\n",
    "                sample.name += str(uuid4())[-6:]\n",
    "                sample.save()\n",
    "        \n",
    "    def iter_trash(self):\n",
    "        for s in self.session.Sample.where({'user_id': self.USER_ID}):\n",
    "            if self.is_trashed(s):\n",
    "                if s.user_id != self.USER_ID:\n",
    "                    raise ValueError(\"Opps! Cannot access other user's samples!\")\n",
    "                yield s\n",
    "    \n",
    "    @classmethod\n",
    "    def clear_field_values(cls, sample):\n",
    "        if not cls.is_trashed(sample):\n",
    "            raise ValueError(\"Opps! Accidentally tried to clear a non trashed sample!\")\n",
    "        for fv in sample.field_values:\n",
    "            print(fv.value)\n",
    "            print(fv.sample)\n",
    "            do_save = False\n",
    "            if fv.value:\n",
    "                fv.value = None\n",
    "                do_save = True\n",
    "            if fv.child_sample_id:\n",
    "                fv.child_sample_id = None\n",
    "                do_save = True\n",
    "            if fv.sample:\n",
    "                fv.sample = None\n",
    "                do_save = True\n",
    "            if do_save:\n",
    "                fv.save()\n",
    "        \n",
    "    def force_clear_field_values(self):\n",
    "        for s in self.trash:\n",
    "            cleared = True\n",
    "            for fv in s.field_values:\n",
    "                if fv.value or fv.sample:\n",
    "                    print(fv.name)\n",
    "                    print('\\t {} {} {}'.format(fv.name, fv.value, fv.sample))\n",
    "                    cleared = False\n",
    "            if not cleared:\n",
    "                print(s.name)\n",
    "                self.clear_field_values(s)\n",
    "        \n",
    "    @property\n",
    "    def trash(self):\n",
    "        if self._trash is None:\n",
    "            samples = list(self.iter_trash())\n",
    "        \n",
    "            with self.session.with_cache() as sess:\n",
    "                sess.browser.update_cache(samples)\n",
    "                sess.browser.get(samples, 'items')\n",
    "                fvs = sess.FieldValue.where({'child_sample_id': [_t.id for _t in samples]})\n",
    "                sess.browser.get(fvs, {'operation', 'parent_sample'})\n",
    "                \n",
    "            self._trash = [s for s in samples if self.valid_for_reuse(s)]\n",
    "        return self._trash\n",
    "    \n",
    "    def _has_items(self, sample):\n",
    "        if sample.items:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def _is_used_in_field_value(self, sample, fvs = None):\n",
    "        with sample.session.with_cache(using_models=True) as sess:\n",
    "            fvs = sess.FieldValue.where({'child_sample_id': sample.id})\n",
    "            for fv in fvs:\n",
    "                if fv.role == 'input':\n",
    "                    return True\n",
    "                elif fv.role == 'output':\n",
    "                    return True\n",
    "            for fv in fvs:\n",
    "                if fv.parent_class == 'Sample':\n",
    "                    if not self.allowed(fv.parent_sample):\n",
    "                        return True\n",
    "                    elif not self.is_trashed(fv.parent_sample):\n",
    "                        return True\n",
    "        \n",
    "    def can_be_trashed(self, sample):\n",
    "        if not self.allowed(sample, do_raise=False):\n",
    "            return False\n",
    "        if not self._is_used_in_field_value(sample):\n",
    "            return False\n",
    "        if not self._has_items(sample):\n",
    "            return False\n",
    "        return True\n",
    "        \n",
    "    def valid_for_reuse(self, sample):\n",
    "        if not self.allowed(sample, do_raise=False):\n",
    "            return False\n",
    "        if not self.is_trashed(sample):\n",
    "            return False\n",
    "#         if self._is_used_in_field_value(sample):\n",
    "#             return False\n",
    "        if self._has_items(sample):\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def reuse(self, sample_type_id):\n",
    "        for s in self.trash:\n",
    "            if s.sample_type_id == sample_type_id and s.id not in self._reused:\n",
    "                if self.valid_for_reuse(s):\n",
    "                    self.clear_field_values(s)\n",
    "                    return s\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sessions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-222a9256f9d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'default'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'aquarium'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrecycler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSampleRecycler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecycler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrash\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sessions' is not defined"
     ]
    }
   ],
   "source": [
    "session = sessions['default']['aquarium']\n",
    "\n",
    "recycler = SampleRecycler(session)\n",
    "\n",
    "for s in recycler.trash:\n",
    "    print(s.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sample(s):\n",
    "    existing = s.session.Sample.find_by_name(s.name)\n",
    "    if not existing:\n",
    "        s.save()\n",
    "    else:\n",
    "        s.id = existing.id\n",
    "    for fv in s.field_values:\n",
    "        if fv.sample:\n",
    "            save_sample(fv.sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def props(s):\n",
    "    if not s.field_values:\n",
    "        return {}\n",
    "    data = s.properties\n",
    "    for k, v in data.items():\n",
    "        if v.__class__.__name__ == 'Sample':\n",
    "            data[k] = v.id\n",
    "    return data\n",
    "            \n",
    "def validate_samples(new_samples):\n",
    "    with session(using_cache=False) as sess:\n",
    "        for s1 in new_samples:\n",
    "            print(s1.name)\n",
    "            s2 = sess.Sample.find_by_name(s1.name)\n",
    "            if not s2:\n",
    "                print(\"{} not saved!\".format(s1.name))\n",
    "                continue\n",
    "            if s1.id != s2.id:\n",
    "                print(\"{} != {}\".format(s1.id, s2.id))\n",
    "            d1 = props(s1)\n",
    "            d2 = props(s2)\n",
    "            d = list(dictdiffer.diff(d1, d2))\n",
    " \n",
    "            if d:\n",
    "                for _d in d:\n",
    "                    print(_d)\n",
    "                return\n",
    "\n",
    "validate_samples(new_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Graph Plotter\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def sorted_center(iterable, reverse=False, key=None, select=None):\n",
    "    centered = []\n",
    "    s = sorted(iterable, key=key, reverse=reverse)\n",
    "    front = False\n",
    "    for v in s:\n",
    "        if not front:\n",
    "            centered.append(v)\n",
    "        else:\n",
    "            centered.insert(0, v)\n",
    "        front = not front\n",
    "    if select:\n",
    "        return [select(_x) for _x in centered]\n",
    "    return centered\n",
    "\n",
    "class GraphPlotter(object):\n",
    "  \n",
    "  def __init__(self, graph, ax=None, pos=None):\n",
    "    if ax is None:\n",
    "      ax = plt.figure(figsize=(3,3)).gca()\n",
    "      ax.axis('off')\n",
    "      ax.set_xlim(0, 1.0)\n",
    "      ax.set_ylim(0, 1.0)\n",
    "    \n",
    "    self._ax = ax\n",
    "    self._graph = graph\n",
    "    if pos is None:\n",
    "      self._pos = dict()\n",
    "      self.spring_layout()\n",
    "    else:\n",
    "      self._pos = pos\n",
    "      \n",
    "  @property\n",
    "  def _base_draw_kwargs(self):\n",
    "    return dict(G=self._graph, ax=self._ax, pos=self._pos)\n",
    " \n",
    "  def topological_sort(self):\n",
    "    xlim = self._ax.get_xlim()\n",
    "    ylim = self._ax.get_ylim()\n",
    "    \n",
    "    x = xlim[1] - xlim[0]\n",
    "    y = ylim[1] - ylim[0]\n",
    "    xspacer = x * 0.05\n",
    "    yspacer = y * 0.05\n",
    "    \n",
    "    new_xlim = (xlim[0] + xspacer, xlim[1] - xspacer)\n",
    "    new_ylim = (ylim[0] + yspacer, ylim[1] - yspacer)\n",
    "    pos = self._topological_sort(self._graph, \n",
    "                                 xlim=new_xlim, \n",
    "                                 ylim=new_ylim)\n",
    "    self._pos = pos\n",
    "\n",
    "  @staticmethod\n",
    "  def _roots_and_leaves(G, include_cycles=False):\n",
    "    roots = [k for k, v in G.in_degree(G.nodes) if v == 0]\n",
    "    leaves = [k for k, v in G.out_degree(G.nodes) if v == 0]\n",
    "    \n",
    "    if include_cycles:\n",
    "      for c in nx.simple_cycles(G):\n",
    "        outdegree = list(G.out_degree(c))\n",
    "        outdegree.sort(key=lambda x: x[1])\n",
    "        leaves.append(outdegree[0][0])\n",
    "\n",
    "        indegree = list(G.out_degree(c))\n",
    "        indegree.sort(key=lambda x: x[1])\n",
    "        roots.append(indegree[0][0])\n",
    "    return roots, leaves\n",
    "  \n",
    "  @classmethod\n",
    "  def _get_roots(cls, G, include_cycles=False):\n",
    "    return cls._roots_and_leaves(G, include_cycles=include_cycles)[0]\n",
    "\n",
    "  @staticmethod\n",
    "  def _get_leaves(G, include_cycles=False):\n",
    "    return cls._roots_and_leaves(G, include_cycles=include_cycles)[1]\n",
    "\n",
    "  @staticmethod\n",
    "  def _get_subgraphs(graph):\n",
    "    \"\"\"Get independent subgraphs\"\"\"\n",
    "    node_list = list(graph.nodes)\n",
    "    subgraphs = []\n",
    "    while len(node_list) > 0:\n",
    "        node = node_list[-1]\n",
    "        subgraph = nx.bfs_tree(to_undirected(graph), node)\n",
    "        for n in subgraph.nodes:\n",
    "            node_list.remove(n)\n",
    "        subgraphs.append(graph.subgraph(subgraph.nodes))\n",
    "    return subgraphs\n",
    "\n",
    "  @classmethod\n",
    "  def _topological_sort(cls, G, xlim=None, ylim=None):\n",
    "    if xlim is None:\n",
    "        xlim = [0.05, 0.95]\n",
    "    if ylim is None:\n",
    "        ylim = [0.05, 0.95]\n",
    "    \n",
    "    rts = cls._get_roots(G, include_cycles=True)\n",
    "    max_depth = {}\n",
    "    for root in rts:\n",
    "        depths = nx.single_source_shortest_path_length(G, root)\n",
    "        for n, d in depths.items():\n",
    "            max_depth[n] = max(max_depth.get(n, d), d)\n",
    "\n",
    "    by_depth = collections.OrderedDict()\n",
    "\n",
    "    for node, depth in max_depth.items():\n",
    "        by_depth.setdefault(depth, [])\n",
    "        by_depth[depth].append(node)\n",
    "    \n",
    "    # center nodes with highest degree\n",
    "    for depth, nodes in by_depth.items():\n",
    "        centered = sorted_center(list(G.degree(nodes)), \n",
    "                                 key=lambda x: x[1], \n",
    "                                 reverse=True,\n",
    "                                select=lambda x: x[0])\n",
    "        by_depth[depth] = centered\n",
    "\n",
    "    # push roots 'up' so they are not stuck on layer one\n",
    "    for root in rts:\n",
    "        successors = list(G.successors(root))\n",
    "        if len(successors) > 0:\n",
    "            min_depth = min([max_depth[s] for s in successors])\n",
    "            max_depth[root] = min_depth - 1\n",
    "\n",
    "    # assign positions\n",
    "\n",
    "    y_min_max = xlim\n",
    "    x_min_max = ylim\n",
    "    max_width = max([len(layer) for layer in by_depth.values()])\n",
    "\n",
    "    y_step = (y_min_max[1] - y_min_max[0]) / (max(by_depth.keys())+1)\n",
    "    x_step = (x_min_max[1] - x_min_max[0]) / (max_width )\n",
    "    positions = {}\n",
    "\n",
    "    for depth in sorted(by_depth):\n",
    "        y = y_step * depth + y_min_max[0]\n",
    "        node_ids = by_depth[depth]\n",
    "        w = len(node_ids)\n",
    "        delta_w = max_width - w\n",
    "        for i, n in enumerate(node_ids):\n",
    "            x_offset = delta_w * x_step / 2.0\n",
    "            x = x_min_max[0] + x_offset + i * x_step\n",
    "            positions[n] = (x, y)\n",
    "    return positions\n",
    "\n",
    "  def spring_layout(self, **kwargs):\n",
    "    pos = nx.spring_layout(self._graph, **kwargs)\n",
    "    self._pos.update(pos)\n",
    "    return self._pos\n",
    "    \n",
    "  @property\n",
    "  def nodes(self):\n",
    "    return self._graph.nodes\n",
    "  \n",
    "  @property\n",
    "  def edges(self):\n",
    "    return self._graph.edges\n",
    "  \n",
    "  def node_attrs(self):\n",
    "    keys = set()\n",
    "    for _, node_data in self._graph.nodes(data=True):\n",
    "      keys.update(set(node_data.keys()))\n",
    "    attrs = {}\n",
    "    for _, node_data in self._graph.nodes(data=True):\n",
    "      for k in keys:\n",
    "        attrs.setdefault(k, list())\n",
    "        attrs[k].append(node_data.get(k, None))\n",
    "    return attrs\n",
    "  \n",
    "  def edge_attrs(self):\n",
    "    keys = set()\n",
    "    for _, _, edge_data in self._graph.edges(data=True):\n",
    "      keys.update(set(edge_data.keys()))\n",
    "    attrs = {}\n",
    "    for _, _, edge_data in self._graph.edges(data=True):\n",
    "      for k in keys:\n",
    "        attrs.setdefault(k, list())\n",
    "        attrs[k].append(edge_data.get(k, None))\n",
    "    return attrs\n",
    "  \n",
    "  def map_edge_attrs(self, attrs, source, target):\n",
    "    vals = [self._normalize(self.edge_attrs()[attr], source, target) for attr in attrs]\n",
    "    return dict(zip(attrs, vals))\n",
    "  \n",
    "  def map_node_attrs(self, attrs, source, target):\n",
    "    vals = [self._normalize(self.node_attrs()[attr], source, target) for attr in attrs]\n",
    "    return dict(zip(attrs, vals))\n",
    "  \n",
    "  def _normalize(arr, source, target):\n",
    "    x = source[1] - source[0]\n",
    "    y = target[1] - target[0]\n",
    "    return [_x/x * y + target[0] for _x in arr] \n",
    "  \n",
    "  def _make_draw_kwargs(self, **kwargs):\n",
    "    kwargs.update(self._base_draw_kwargs)\n",
    "    return kwargs\n",
    "  \n",
    "  def _draw(self, draw_function, zorder=None, **kwargs):\n",
    "    draw_kwargs = self._make_draw_kwargs(**kwargs)\n",
    "    collection = draw_function(**draw_kwargs)\n",
    "    if collection is not None and zorder is not None:\n",
    "      try:\n",
    "        # This is for compatibility with older matplotlib.\n",
    "        collection.set_zorder(zorder)\n",
    "      except AttributeError:\n",
    "        # This is for compatibility with newer matplotlib.\n",
    "        collection[0].set_zorder(zorder)\n",
    "    return collection\n",
    "\n",
    "  def draw_nodes(self, **kwargs):\n",
    "    \"\"\"Useful kwargs: nodelist, node_size, node_color, linewidths.\"\"\"\n",
    "    if (\"node_color\" in kwargs and\n",
    "        isinstance(kwargs[\"node_color\"], collections.Sequence) and\n",
    "        len(kwargs[\"node_color\"]) in {3, 4} and\n",
    "        not isinstance(kwargs[\"node_color\"][0],\n",
    "                       (collections.Sequence, np.ndarray))):\n",
    "      num_nodes = len(kwargs.get(\"nodelist\", self.nodes))\n",
    "      kwargs[\"node_color\"] = np.tile(\n",
    "          np.array(kwargs[\"node_color\"])[None], [num_nodes, 1])\n",
    "    return self._draw(nx.draw_networkx_nodes, **kwargs)\n",
    "\n",
    "  def draw_edges(self, **kwargs):\n",
    "    \"\"\"Useful kwargs: edgelist, width.\"\"\"\n",
    "    return self._draw(nx.draw_networkx_edges, **kwargs)\n",
    "  \n",
    "  def draw_graph(self,\n",
    "                 node_size=200,\n",
    "                 node_color=(0.4, 0.8, 0.4),\n",
    "                 node_linewidth=1.0,\n",
    "                 edge_width=1.0):\n",
    "    \n",
    "    node_border_color = (0.0, 0.0, 0.0, 1.0)\n",
    "    \n",
    "    # Plot nodes.\n",
    "    self.draw_nodes(\n",
    "        nodelist=self.nodes,\n",
    "        node_size=node_size,\n",
    "        node_color=node_color,\n",
    "        linewidths=node_linewidth,\n",
    "        edgecolors=node_border_color,\n",
    "        zorder=20)\n",
    "    # Plot edges.\n",
    "    self.draw_edges(edgelist=self.edges, width=edge_width, zorder=10)\n",
    "\n",
    "  \n",
    "  \n",
    "g = nx.balanced_tree(2, 2)\n",
    "g = nx.to_directed(g)\n",
    "\n",
    "for e in g.edges:\n",
    "  g.edges[e[0], e[1]]['weight'] = 1\n",
    "  \n",
    "ax = plt.figure(figsize=(3,3)).gca()\n",
    "ax.axis('off')\n",
    "ax.set_xlim(0, 1.0)\n",
    "ax.set_ylim(0, 1.0)\n",
    "    \n",
    "plotter = GraphPlotter(g, ax=ax)\n",
    "plotter.topological_sort()\n",
    "\n",
    "plotter.draw_graph(node_color=(0, 0, 0), edge_width=plotter.edge_attrs()['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydent import Planner\n",
    "import networkx as nx\n",
    "%matplotlib inline\n",
    "\n",
    "g = nx.DiGraph()\n",
    "for qk, d in results['designs'].items():\n",
    "    if not len(d['assemblies']):\n",
    "        print(\"MISSING ASSEMBLY FOR {}\".format(d.keys()))\n",
    "        continue\n",
    "    reaction_id = d['assemblies'][0]['final_assembly_reaction'][0]\n",
    "    \n",
    "    assembly_reaction = results['reactions'][reaction_id]\n",
    "    in_samples = [results['molecules'][i]['__sample__'] for i in assembly_reaction['inputs']]\n",
    "    out_samples = [results['molecules'][i]['__sample__'] for i in assembly_reaction['outputs']]\n",
    "    print(out_samples[0].name)\n",
    "    for s in in_samples:\n",
    "        print('\\t{}'.format(s.name))\n",
    "        \n",
    "        g.add_edge(s.name, out_samples[0].name)\n",
    "\n",
    "plotter = GraphPlotter(g)\n",
    "plotter.topological_sort()\n",
    "\n",
    "plotter.draw_graph()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if updating samples exist\n",
    "\n",
    "exists = []\n",
    "for u in to_update:\n",
    "    print(u.id)\n",
    "    existing = session.Sample.find_by_name(u.name)\n",
    "    if existing:\n",
    "        print('{} exists'.format(u.name))\n",
    "        exists.append(existing)\n",
    "        \n",
    "if exists:\n",
    "    raise ValueError\n",
    "#     assert not session.Sample.find_by_name(u.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
